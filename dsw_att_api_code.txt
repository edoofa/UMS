import requests
import pandas as pd
from datetime import datetime, timezone
import json
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# Replace with your actual details
EDOOFY_API_ENDPOINT = "https://edoofa-portal.bubbleapps.io/version-test/api/1.1/obj"
UMS_API_ENDPOINT = "https://app.edoofa.com/version-test/api/1.1/obj"
EDOOFY_API_TOKEN = "4014977c2d22a6aeba2fe9a495b2b0ed"
UMS_API_TOKEN = "786720e8eb68de7054d1149b56cc04f9"

def get_headers(endpoint):
    token = EDOOFY_API_TOKEN if "edoofa-portal" in endpoint else UMS_API_TOKEN
    return {'Authorization': f'Bearer {token}'}

def iso_format(date_string):
    """Converts a date string to ISO 8601 format with time and timezone."""
    date = datetime.strptime(date_string, "%Y-%m-%d")
    return date.replace(tzinfo=timezone.utc).isoformat()

# Define start_date and end_date in ISO 8601 format
start_date = iso_format('2024-01-14')
end_date = iso_format('2024-01-23')

def fetch_all_data(start_date, end_date, endpoint, is_engagement_data):
    all_data = []
    cursor = 0
    limit = 100
    total_records = None
    headers = get_headers(endpoint)

    while total_records is None or cursor < total_records:
        try:
            constraints = []
            if is_engagement_data:
                # Add constraints for date range and engagement type
                constraints = [
                    {'key': 'engagement-date', 'constraint_type': 'greater than', 'value': start_date},
                    {'key': 'engagement-date', 'constraint_type': 'less than', 'value': end_date},
                    {'key': 'engagement-type', 'constraint_type': 'equals', 'value': 'DSW'}
                ]

            params = {
                'constraints': json.dumps(constraints),
                'cursor': cursor,
                'limit': limit
            }
            response = requests.get(endpoint, headers=headers, params=params)
            response.raise_for_status()

            data = response.json()
            records = data['response']['results']
            all_data.extend(records)
            
            if total_records is None:
                total_records = data['response']['remaining'] + len(records)
            cursor += len(records)

            logging.info(f"Fetched {len(records)} records. Total fetched: {cursor}.")

        except requests.exceptions.HTTPError as err:
            logging.error(f"HTTP error occurred: {err}")
            break
        except Exception as err:
            logging.error(f"Error occurred: {err}")
            break

    return all_data

# Fetch engagement data
engagement_data = fetch_all_data(start_date, end_date, f"{EDOOFY_API_ENDPOINT}/engagement", True)

# Fetch dsw-session data without date constraints
dsw_session_data = fetch_all_data(start_date, end_date, f"{UMS_API_ENDPOINT}/dsw-session", False)

# Create DataFrames from the fetched data
dsw_session_df = pd.DataFrame(dsw_session_data)
engagement_df = pd.DataFrame(engagement_data)

def load_previous_entries():
    # Implement the logic to load previously processed entries from a file or database
    # You can use a file or database connection here
    # Return a set of unique entry keys (date, presence, ewyl-group-name)

    # For example, if you are using a file to store previous entries:
    try:
        with open('previous_entries.txt', 'r') as file:
            lines = file.readlines()
            previous_entries = set(tuple(line.strip().split(',')) for line in lines)
    except FileNotFoundError:
        previous_entries = set()

    return previous_entries

def update_previous_entries(new_entries):
    # Implement the logic to update the file or database with new unique entries
    # You can use a file or a database connection here

    # For example, if you are using a file to store previous entries:
    with open('previous_entries.txt', 'a') as file:
        for entry in new_entries:
            file.write(','.join(str(e) for e in entry) + '\n')

def is_duplicate_entry(new_entry, existing_data):
    for entry in existing_data:
        if (entry['date'] == new_entry['date'] and 
            entry['present'] == new_entry['present'] and 
            entry['ewyl-group-name'] == new_entry['ewyl-group-name']):
            return True
    return False


def fetch_and_filter_dsw_session_data(start_date, end_date):
    dsw_session_data = fetch_all_data(start_date, end_date, f"{UMS_API_ENDPOINT}/dsw-session", False)

    # Load previously processed entries (if any) from a file or database
    previous_entries = load_previous_entries()

    unique_entries = []
    new_entries = []

    for entry in dsw_session_data:
        entry_key = (entry['date'], entry['present'], entry['ewyl-group-name'])
        
        if entry_key not in previous_entries:
            if not is_duplicate_entry(entry, dsw_session_data):
                unique_entries.append(entry_key)
                new_entries.append(entry)
            else:
                logging.warning(f"Duplicate entry found: {entry}")

    # Update the file or database with the new unique entries
    update_previous_entries(unique_entries)

    return new_entries

# Fetch and filter dsw-session data for new entries
new_dsw_session_data = fetch_and_filter_dsw_session_data(start_date, end_date)

def fetch_student_data():
    all_students = []
    cursor = 0
    limit = 100
    more_records = True  # Flag to indicate if more records are available
    student_endpoint = f"{EDOOFY_API_ENDPOINT}/student"
    headers = get_headers(student_endpoint)

    while more_records:
        params = {
            'cursor': cursor,
            'limit': limit
        }
        response = requests.get(student_endpoint, headers=headers, params=params)
        if response.status_code == 200:
            data = response.json()['response']
            fetched_students = data['results']
            all_students.extend(fetched_students)
            cursor += len(fetched_students)  # Update the cursor
            more_records = len(fetched_students) == limit  # Check if we've fetched less than the limit
            logging.info(f"Fetched {len(fetched_students)} student records. Total fetched: {len(all_students)}")
        else:
            logging.error(f"Failed to fetch student data: {response.status_code} {response.text}")
            break

    return all_students

# After fetching the student data
student_data = fetch_student_data()

def is_duplicate_entry(new_entry, existing_data):
    new_entry_date = datetime.strptime(new_entry['date'], "%Y-%m-%dT%H:%M:%S.%fZ").date()

    for entry in existing_data:
        existing_entry_date = datetime.strptime(entry['date'], "%Y-%m-%dT%H:%M:%S.%fZ").date()
        
        if (existing_entry_date == new_entry_date and 
            entry['present'] == new_entry['present'] and 
            entry['ewyl-group-name'] == new_entry['ewyl-group-name']):
            return True
    return False

def map_engagement_to_dsw(engagement_data, student_data, dsw_session_data):
    student_info = {student['_id']: student for student in student_data}
    logging.info(f"Student info keys: {list(student_info.keys())[:5]}")  # Print the first 5 keys for debug

    processed_entries = []  # Use a list to store processed entries

    for entry in engagement_data:
        student_id = entry.get('student')
        if student_id in student_info:
            student = student_info[student_id]
            processed_entry = {
                'date': entry.get('engagement-date'),
                'college': student.get('college'),
                'dsw-officer': student.get('dsw-officer'),
                'present': entry.get('daily-attendance'),
                'ewyl-group-name': student.get('EWYL-group-name'),
                'admissions-group-name': student.get('KAM-group-name'),
                'Duplicate': 'no'  # Default value for Duplicate key
            }

            # Check if the entry is a duplicate
            if is_duplicate_entry(processed_entry, dsw_session_data):
                processed_entry['Duplicate'] = 'yes'
                logging.warning(f"Duplicate entry found for Student ID {student_id}.")
            else:
                processed_entries.append(processed_entry)  # Append the processed entry to the list

        else:
            logging.warning(f"Student ID {student_id} from engagement data not found in student info.")

    return processed_entries

# Map and process the engagement data
processed_data = map_engagement_to_dsw(engagement_data, student_data, dsw_session_data)

# Ensure that processed_data is not empty
if not processed_data:
    logging.error("No processed data to save. Please check the engagement data and student data for consistency.")


# Fetch the student data
student_data = fetch_student_data()

def update_previous_entries(new_entries):
    # Implement the logic to update the file or database with new unique entries
    # You can use a file or a database connection here

    # For example, if you are using a file to store previous entries:
    with open('previous_entries.txt', 'a') as file:
        for entry in new_entries:
            file.write(','.join(str(e) for e in entry) + '\n')




def fetch_and_filter_dsw_session_data(start_date, end_date):
    dsw_session_data = fetch_all_data(start_date, end_date, f"{UMS_API_ENDPOINT}/dsw-session", False)

    # Load previously processed entries (if any) from a file or database
    previous_entries = load_previous_entries()

    unique_entries = []
    new_entries = []

    for entry in dsw_session_data:
        entry_key = (entry['date'], entry['present'], entry['ewyl-group-name'])

        is_duplicate = is_duplicate_entry(entry, dsw_session_data)
        entry['Duplicate'] = 'yes' if is_duplicate else 'no'

        if entry_key not in previous_entries:
            unique_entries.append(entry_key)
            new_entries.append(entry)

    # Update the file or database with the new unique entries
    update_previous_entries(unique_entries)

    return new_entries

# Ensure that processed_data is not empty
if not processed_data:
    logging.error("No processed data to save. Please check the engagement data and student data for consistency.")

# After processing all data and checking for duplicates
processed_data_with_duplicates = processed_data  # This includes all entries with 'yes' and 'no' in the 'Duplicate' column

# Filter out non-duplicate entries for a separate sheet
non_duplicate_entries = [entry for entry in processed_data if entry['Duplicate'] == 'no']


# Save to Excel at the specified location, including the processed data
excel_file_path = "C:\\Users\\ayush\\Documents\\UMS\\dsw-att_data.xlsx"
with pd.ExcelWriter(excel_file_path) as writer:
    pd.DataFrame(engagement_data).to_excel(writer, sheet_name='Engagement Data')
    pd.DataFrame(dsw_session_data).to_excel(writer, sheet_name='DSW-Session Data')
    pd.DataFrame(processed_data_with_duplicates).to_excel(writer, sheet_name='Processed Data')
    pd.DataFrame(student_data).to_excel(writer, sheet_name='Student Data')  # This line adds the student data sheet
    pd.DataFrame(non_duplicate_entries).to_excel(writer, sheet_name='Processed Data (Non-Duplicate)')

logging.info(f"Data fetched and saved at {excel_file_path}")

def upload_data_to_dsw_session(processed_data, endpoint):
    headers = get_headers(endpoint)
    headers['Content-Type'] = 'application/json'  # Set the content type to application/json
    upload_url = f"{endpoint}/dsw-session"

    # Prepare the entries in the format expected by the API
    entries_to_upload = [{
        'admissions-group-name': entry['admissions-group-name'],
        'college': entry['college'],
        'date': entry['date'],
        'dsw-officer': entry.get('dsw-officer', ''),  # Provide a default empty string if not present
        'ewyl-group-name': entry['ewyl-group-name'],
        'present': 'yes' if entry['present'] else 'no',  # Convert boolean to 'yes'/'no'
        # 'session-feedback' and 'session-rating' might be optional or provided by another process
    } for entry in processed_data if entry['Duplicate'] == 'no']

    for entry in entries_to_upload:
        data_to_upload = json.dumps(entry)  # Convert the entry to a JSON formatted string
        try:
            response = requests.post(upload_url, headers=headers, data=data_to_upload)
            response.raise_for_status()
            logging.info(f"Successfully uploaded entry: {entry}")
        except requests.exceptions.HTTPError as err:
            logging.error(f"HTTP error occurred: {err} - Entry: {entry}")
        except Exception as err:
            logging.error(f"An error occurred: {err} - Entry: {entry}")

# Call the upload function with the processed data
upload_data_to_dsw_session(processed_data, UMS_API_ENDPOINT)

# Clear the DataFrames to release memory
dsw_session_df = None
engagement_df = None